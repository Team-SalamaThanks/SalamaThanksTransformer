{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AdamW\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Peter\\SalamaThanks\\.env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Importing SalamaThanks Transformer v1 for English-to-Filipino Translation\n",
    "\n",
    "model_v1_checkpoint_en2fil = \"./v1/en_to_fil/v1.0\"\n",
    "tokenizer_v1_en2fil = AutoTokenizer.from_pretrained(model_v1_checkpoint_en2fil, return_tensors=\"tf\")\n",
    "model_v1_en2fil = AutoModelForSeq2SeqLM.from_pretrained(model_v1_checkpoint_en2fil)\n",
    "data_collator_v1_en2fil = DataCollatorForSeq2Seq(tokenizer_v1_en2fil, model=model_v1_en2fil)\n",
    "optimizer_v1_en2fil = AdamW(model_v1_en2fil.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing SalamaThanks Transformer v1 for Filipino-to-English Translation\n",
    "\n",
    "model_v1_checkpoint_fil2en = \"./v1/fil_to_en/v1.0\"\n",
    "tokenizer_v1_fil2en = AutoTokenizer.from_pretrained(model_v1_checkpoint_fil2en, return_tensors=\"tf\")\n",
    "model_v1_fil2en = AutoModelForSeq2SeqLM.from_pretrained(model_v1_checkpoint_fil2en)\n",
    "data_collator_v1_fil2en = DataCollatorForSeq2Seq(tokenizer_v1_fil2en, model=model_v1_fil2en)\n",
    "optimizer_v1_fil2en = AdamW(model_v1_fil2en.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading sacreBLEU to evaluate BLEU Score and defining length of dataset sentences.\n",
    "\n",
    "metric = load_metric('sacrebleu')\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset bible_para (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\bible_para\\en-tl-lang1=en,lang2=tl\\0.0.0\\b6cc20bcbfb0299beeba1dcc80a8420b975938ca0eef75b3ed30b50df7d950b1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc6e3413f2421bbeae0b7d7f3b026b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading bible_para dataset from Huggingface\n",
    "\n",
    "raw_bible_dataset = load_dataset('bible_para', lang1='en', lang2='tl')\n",
    "raw_bible_dataset = raw_bible_dataset.remove_columns(['id'])\n",
    "bible_dataset = raw_bible_dataset['train'].train_test_split(train_size=0.92, test_size=0.08)\n",
    "bible_dataset[\"validation\"] = bible_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset tatoeba (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\tatoeba\\en-tl-lang1=en,lang2=tl\\0.0.0\\b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968a381f16724f999df667e51aaeac38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2014-e2e7a3ad21b13d2b\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2014-e2e7a3ad21b13d2b\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242551027a214d3b9f39d95cd43fdcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2015-e6db70baf321c7f2\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2015-e6db70baf321c7f2\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0102fecd08594436a16b6890e887d7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2016-9907aa0a67465528\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2016-9907aa0a67465528\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7746d49353f54864ad04b5b1e2cc0397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_gnome (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\opus_gnome\\en-tl-lang1=en,lang2=tl\\0.0.0\\c00e5dfb1b3b508d7898e160feee1d391e67a3651a06570b45d54ab6a8886217)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8741a882c4cd46faa31897a80d908731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_para_crawl (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\opus_para_crawl\\en-tl-lang1=en,lang2=tl\\0.0.0\\d0becb3ac754eb295ccf6b4b87f391d12d2f4217dbc4f87f2a9718ba1f2de4a3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab06c8f4f83418986722076f431bdab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset open_subtitles (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\open_subtitles\\en-tl-lang1=en,lang2=tl\\0.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bc0614efa8456bab75973cc563e22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_ubuntu (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\opus_ubuntu\\en-tl-lang1=en,lang2=tl\\0.0.0\\7ac83b46edf6d0b6ff96bc86d5aadfb8b877c2f136a94af490988c442d3814b8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cfcafac1cf47909c54289a01adf679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset multi_para_crawl (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\multi_para_crawl\\en-tl-lang1=en,lang2=tl\\0.0.0\\923bd780ac54acc2d7228bf36806e2a2309aaab30aa3bee613145aaff39eb83c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2025593f539c443cb564236374e29b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset qed_amara (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\qed_amara\\en-tl-lang1=en,lang2=tl\\0.0.0\\3662cb8fbbe21ebafc420ac0a1b3d1898312661d4f898adc79149fa09d073ba0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f2829613354dd5a2769d0c7c63ab54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading datasets from Huggingface and combining them into one dataset\n",
    "\n",
    "raw_tatoeba_dataset = load_dataset('tatoeba', lang1='en', lang2='tl')\n",
    "raw_tatoeba_dataset = raw_tatoeba_dataset.remove_columns(['id'])\n",
    "raw_tedtalks1_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2014\")\n",
    "raw_tedtalks2_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2015\")\n",
    "raw_tedtalks3_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2016\")\n",
    "raw_gnome_dataset = load_dataset('opus_gnome', lang1='en', lang2='tl')\n",
    "raw_gnome_dataset = raw_gnome_dataset.remove_columns(['id'])\n",
    "raw_paracrawl_dataset = load_dataset('opus_paracrawl', lang1='en', lang2='tl')\n",
    "raw_paracrawl_dataset = raw_paracrawl_dataset.remove_columns(['id'])\n",
    "raw_subtitles_dataset = load_dataset(\"open_subtitles\", lang1=\"en\", lang2=\"tl\")\n",
    "raw_subtitles_dataset = raw_subtitles_dataset.remove_columns(['id'])\n",
    "raw_subtitles_dataset = raw_subtitles_dataset.remove_columns(['meta'])\n",
    "raw_ubuntu_dataset = load_dataset('opus_ubuntu', lang1='en', lang2='tl')\n",
    "raw_ubuntu_dataset = raw_ubuntu_dataset.remove_columns(['id'])\n",
    "raw_multiparacrawl_dataset = load_dataset('multi_para_crawl', lang1='en', lang2='tl')\n",
    "raw_multiparacrawl_dataset = raw_multiparacrawl_dataset.remove_columns(['id'])\n",
    "raw_qedamara_dataset = load_dataset('qed_amara', lang1 = 'en', lang2 = 'tl')\n",
    "raw_qedamara_dataset = raw_qedamara_dataset.remove_columns(['id'])\n",
    "\n",
    "\n",
    "raw_combined_dataset = concatenate_datasets([raw_tatoeba_dataset['train'], raw_tedtalks1_dataset['train'], raw_tedtalks2_dataset['train'], raw_tedtalks3_dataset['train'], raw_gnome_dataset['train'], raw_paracrawl_dataset['train'], raw_subtitles_dataset['train'], raw_ubuntu_dataset['train'], raw_multiparacrawl_dataset['train'], raw_qedamara_dataset['train']])\n",
    "\n",
    "combined_dataset = raw_combined_dataset.train_test_split(train_size=0.92, test_size=0.08)\n",
    "combined_dataset[\"validation\"] = combined_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 57219\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 4976\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 489822\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 42594\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2764a709029b4a47a93587c9fdaaaf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd92daefd1f1435bb486093eb7a11255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d84c1ac21b3455fa97151a34a8a113d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 61.53\n",
      "Precisions: [81.77472363310427, 66.14824733065541, 55.671678930717, 47.606412382531786]\n",
      "Brevity Penalty: 1.00\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v1 Transformer (English-to-Filipino) using Bible Dataset\n",
    "\n",
    "def preprocess_function1(bibledataset):\n",
    "    inputs = [x[\"en\"] for x in bibledataset[\"translation\"]]\n",
    "    targets = [y[\"tl\"] for y in bibledataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v1_en2fil(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v1_en2fil.as_target_tokenizer():\n",
    "        labels = tokenizer_v1_en2fil(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_bible_dataset1 = bible_dataset.map(\n",
    "    preprocess_function1,\n",
    "    batched=True,\n",
    "    remove_columns=bible_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_bible_dataset1.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v1_en2fil = DataLoader(\n",
    "    tokenized_bible_dataset1[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v1_en2fil, batch_size=48\n",
    ")\n",
    "\n",
    "model_v1_en2fil, optimizer_v1_en2fil, eval_dataloader_v1_en2fil = accelerator.prepare(\n",
    "    model_v1_en2fil, optimizer_v1_en2fil, eval_dataloader_v1_en2fil\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v1_en2fil.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v1_en2fil.pad_token_id)\n",
    "    decoded_labels = tokenizer_v1_en2fil.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v1_en2fil.eval()\n",
    "for batch in tqdm(eval_dataloader_v1_en2fil):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v1_en2fil).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v1_en2fil.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v1_en2fil = metric.compute()\n",
    "print(f\"BLEU score: {results_v1_en2fil['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v1_en2fil['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v1_en2fil['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6725c4f3890b4f268623e4dd8b8ee6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3904e02e72c94c27baca72407eb9db35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd1bf6d6cdb4f429252c595391f0c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 55.88\n",
      "Precisions: [78.89580533352196, 62.07933132927634, 50.81044060573777, 42.3317250652535]\n",
      "Brevity Penalty: 0.98\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v1 Transformer (Filipino-to-English) using Bible Dataset\n",
    "\n",
    "def preprocess_function2(bibledataset):\n",
    "    inputs = [x[\"tl\"] for x in bibledataset[\"translation\"]]\n",
    "    targets = [y[\"en\"] for y in bibledataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v1_fil2en(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v1_fil2en.as_target_tokenizer():\n",
    "        labels = tokenizer_v1_fil2en(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_bible_dataset2 = bible_dataset.map(\n",
    "    preprocess_function2,\n",
    "    batched=True,\n",
    "    remove_columns=bible_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_bible_dataset2.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v1_fil2en = DataLoader(\n",
    "    tokenized_bible_dataset2[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v1_fil2en, batch_size=48\n",
    ")\n",
    "\n",
    "model_v1_fil2en, optimizer_v1_fil2en, eval_dataloader_v1_fil2en = accelerator.prepare(\n",
    "    model_v1_fil2en, optimizer_v1_fil2en, eval_dataloader_v1_fil2en\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v1_fil2en.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v1_fil2en.pad_token_id)\n",
    "    decoded_labels = tokenizer_v1_fil2en.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v1_fil2en.eval()\n",
    "for batch in tqdm(eval_dataloader_v1_fil2en):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v1_fil2en).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v1_fil2en.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v1_fil2en = metric.compute()\n",
    "print(f\"BLEU score: {results_v1_fil2en['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v1_fil2en['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v1_fil2en['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775ed80179554c688aa50cb3b7a95d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43810043de44d938b024f6b10e538f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c354d0c1d345cf9ae276588a3ad3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 30.15\n",
      "Precisions: [60.50504185988609, 35.203135247798045, 23.512256903361276, 16.493135777304666]\n",
      "Brevity Penalty: 1.00\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v1 Transformer (English-to-Filipino) using Combined Dataset\n",
    "\n",
    "def preprocess_function3(combineddataset):\n",
    "    inputs = [x[\"en\"] for x in combineddataset[\"translation\"]]\n",
    "    targets = [y[\"tl\"] for y in combineddataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v1_en2fil(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v1_en2fil.as_target_tokenizer():\n",
    "        labels = tokenizer_v1_en2fil(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_combined_dataset1 = combined_dataset.map(\n",
    "    preprocess_function3,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_combined_dataset1.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v1_en2fil = DataLoader(\n",
    "    tokenized_combined_dataset1[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v1_en2fil, batch_size=48\n",
    ")\n",
    "\n",
    "model_v1_en2fil, optimizer_v1_en2fil, eval_dataloader_v1_en2fil = accelerator.prepare(\n",
    "    model_v1_en2fil, optimizer_v1_en2fil, eval_dataloader_v1_en2fil\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v1_en2fil.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v1_en2fil.pad_token_id)\n",
    "    decoded_labels = tokenizer_v1_en2fil.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v1_en2fil.eval()\n",
    "for batch in tqdm(eval_dataloader_v1_en2fil):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v1_en2fil).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v1_en2fil.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v1_en2fil = metric.compute()\n",
    "print(f\"BLEU score: {results_v1_en2fil['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v1_en2fil['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v1_en2fil['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3745742a007b4505a5fcbe2f9bac8e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f6f00c293f47a9a2ac2bf2c63d0946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0890983217e43149402b36d7d8b2c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 32.81\n",
      "Precisions: [64.41621315994063, 40.13769995180502, 27.748246003658437, 19.790997984623424]\n",
      "Brevity Penalty: 0.95\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v1 Transformer (Filipino-to-English) using Combined Dataset\n",
    "\n",
    "def preprocess_function4(combineddataset):\n",
    "    inputs = [x[\"tl\"] for x in combineddataset[\"translation\"]]\n",
    "    targets = [y[\"en\"] for y in combineddataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v1_fil2en(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v1_fil2en.as_target_tokenizer():\n",
    "        labels = tokenizer_v1_fil2en(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_combined_dataset2 = combined_dataset.map(\n",
    "    preprocess_function4,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_combined_dataset2.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v1_fil2en = DataLoader(\n",
    "    tokenized_combined_dataset2[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v1_fil2en, batch_size=48\n",
    ")\n",
    "\n",
    "model_v1_fil2en, optimizer_v1_fil2en, eval_dataloader_v1_fil2en = accelerator.prepare(\n",
    "    model_v1_fil2en, optimizer_v1_fil2en, eval_dataloader_v1_fil2en\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v1_fil2en.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v1_fil2en.pad_token_id)\n",
    "    decoded_labels = tokenizer_v1_fil2en.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v1_fil2en.eval()\n",
    "for batch in tqdm(eval_dataloader_v1_fil2en):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v1_fil2en).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v1_fil2en.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v1_fil2en = metric.compute()\n",
    "print(f\"BLEU score: {results_v1_fil2en['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v1_fil2en['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v1_fil2en['bp']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05a3867fc7cb355168cd3069bbfa61f57b49db580b0a4f5d5e33f8a392c67a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
