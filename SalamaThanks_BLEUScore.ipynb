{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AdamW\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Peter\\SalamaThanks\\.env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Importing SalamaThanks Transformer v3 for English-to-Filipino Translation\n",
    "\n",
    "model_v3_checkpoint_en2fil = \"SalamaThanks/SalamaThanksTransformer_en2fil_v3\"\n",
    "tokenizer_v3_en2fil = AutoTokenizer.from_pretrained(model_v3_checkpoint_en2fil, return_tensors=\"tf\")\n",
    "model_v3_en2fil = AutoModelForSeq2SeqLM.from_pretrained(model_v3_checkpoint_en2fil)\n",
    "data_collator_v3_en2fil = DataCollatorForSeq2Seq(tokenizer_v3_en2fil, model=model_v3_en2fil)\n",
    "optimizer_v3_en2fil = AdamW(model_v3_en2fil.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing SalamaThanks Transformer v3 for Filipino-to-English Translation\n",
    "\n",
    "model_v3_checkpoint_fil2en = \"SalamaThanks/SalamaThanksTransformer_fil2en_v3\"\n",
    "tokenizer_v3_fil2en = AutoTokenizer.from_pretrained(model_v3_checkpoint_fil2en, return_tensors=\"tf\")\n",
    "model_v3_fil2en = AutoModelForSeq2SeqLM.from_pretrained(model_v3_checkpoint_fil2en)\n",
    "data_collator_v3_fil2en = DataCollatorForSeq2Seq(tokenizer_v3_fil2en, model=model_v3_fil2en)\n",
    "optimizer_v3_fil2en = AdamW(model_v3_fil2en.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading sacreBLEU to evaluate BLEU Score and defining length of dataset sentences.\n",
    "\n",
    "metric = load_metric('sacrebleu')\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset bible_para (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\bible_para\\en-tl-lang1=en,lang2=tl\\0.0.0\\b6cc20bcbfb0299beeba1dcc80a8420b975938ca0eef75b3ed30b50df7d950b1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e7b21d05024f2cbb02b2357ab8760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading bible_para dataset from Huggingface\n",
    "\n",
    "raw_bible_dataset = load_dataset('bible_para', lang1='en', lang2='tl')\n",
    "raw_bible_dataset = raw_bible_dataset.remove_columns(['id'])\n",
    "bible_dataset = raw_bible_dataset['train'].train_test_split(train_size=0.92, test_size=0.08)\n",
    "bible_dataset[\"validation\"] = bible_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset tatoeba (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\tatoeba\\en-tl-lang1=en,lang2=tl\\0.0.0\\b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e565f5f0d168406fa205f0a7e7037d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2014-e2e7a3ad21b13d2b\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2014-e2e7a3ad21b13d2b\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f68f514815d4d49beabc91e82f27088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2015-e6db70baf321c7f2\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2015-e6db70baf321c7f2\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc283f1869e4e3cb851d62000ee9cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2016-9907aa0a67465528\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2016-9907aa0a67465528\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefbb826b9c24606959bc996e7cd743c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_gnome (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\opus_gnome\\en-tl-lang1=en,lang2=tl\\0.0.0\\c00e5dfb1b3b508d7898e160feee1d391e67a3651a06570b45d54ab6a8886217)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b59c9b50a054ec7b1c8c8a27fce1dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_para_crawl (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\opus_para_crawl\\en-tl-lang1=en,lang2=tl\\0.0.0\\d0becb3ac754eb295ccf6b4b87f391d12d2f4217dbc4f87f2a9718ba1f2de4a3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92150a7e48e64583a19474fdf2065e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset open_subtitles (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\open_subtitles\\en-tl-lang1=en,lang2=tl\\0.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e79c30dded4945b0be1e9ef96b31d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_ubuntu (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\opus_ubuntu\\en-tl-lang1=en,lang2=tl\\0.0.0\\7ac83b46edf6d0b6ff96bc86d5aadfb8b877c2f136a94af490988c442d3814b8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbade9fc9e9a4529bc09d5e7428f4d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset multi_para_crawl (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\multi_para_crawl\\en-tl-lang1=en,lang2=tl\\0.0.0\\923bd780ac54acc2d7228bf36806e2a2309aaab30aa3bee613145aaff39eb83c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c97abe0f8224ab1ae904b1c2220f558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset qed_amara (C:\\Users\\Mary\\.cache\\huggingface\\datasets\\qed_amara\\en-tl-lang1=en,lang2=tl\\0.0.0\\3662cb8fbbe21ebafc420ac0a1b3d1898312661d4f898adc79149fa09d073ba0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f7a62cff3c4b218a1be4b87aa687ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading datasets from Huggingface and combining them into one dataset\n",
    "\n",
    "raw_tatoeba_dataset = load_dataset('tatoeba', lang1='en', lang2='tl')\n",
    "raw_tatoeba_dataset = raw_tatoeba_dataset.remove_columns(['id'])\n",
    "raw_tedtalks1_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2014\")\n",
    "raw_tedtalks2_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2015\")\n",
    "raw_tedtalks3_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2016\")\n",
    "raw_gnome_dataset = load_dataset('opus_gnome', lang1='en', lang2='tl')\n",
    "raw_gnome_dataset = raw_gnome_dataset.remove_columns(['id'])\n",
    "raw_paracrawl_dataset = load_dataset('opus_paracrawl', lang1='en', lang2='tl')\n",
    "raw_paracrawl_dataset = raw_paracrawl_dataset.remove_columns(['id'])\n",
    "raw_subtitles_dataset = load_dataset(\"open_subtitles\", lang1=\"en\", lang2=\"tl\")\n",
    "raw_subtitles_dataset = raw_subtitles_dataset.remove_columns(['id'])\n",
    "raw_subtitles_dataset = raw_subtitles_dataset.remove_columns(['meta'])\n",
    "raw_ubuntu_dataset = load_dataset('opus_ubuntu', lang1='en', lang2='tl')\n",
    "raw_ubuntu_dataset = raw_ubuntu_dataset.remove_columns(['id'])\n",
    "raw_multiparacrawl_dataset = load_dataset('multi_para_crawl', lang1='en', lang2='tl')\n",
    "raw_multiparacrawl_dataset = raw_multiparacrawl_dataset.remove_columns(['id'])\n",
    "raw_qedamara_dataset = load_dataset('qed_amara', lang1 = 'en', lang2 = 'tl')\n",
    "raw_qedamara_dataset = raw_qedamara_dataset.remove_columns(['id'])\n",
    "\n",
    "\n",
    "raw_combined_dataset = concatenate_datasets([raw_tatoeba_dataset['train'], raw_tedtalks1_dataset['train'], raw_tedtalks2_dataset['train'], raw_tedtalks3_dataset['train'], raw_gnome_dataset['train'], raw_paracrawl_dataset['train'], raw_subtitles_dataset['train'], raw_ubuntu_dataset['train'], raw_multiparacrawl_dataset['train'], raw_qedamara_dataset['train']])\n",
    "\n",
    "combined_dataset = raw_combined_dataset.train_test_split(train_size=0.92, test_size=0.08)\n",
    "combined_dataset[\"validation\"] = combined_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 57219\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 4976\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 489822\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 42594\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d58107f51154010a98a4073c952c3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff307ba71b94f118bdc44df89af9779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f64e041c5746ffa9f7a2c5a1c5fee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 67.34\n",
      "Precisions: [84.52908273788354, 71.41950355794609, 62.17990807616546, 54.79403027808597]\n",
      "Brevity Penalty: 1.00\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v3 Transformer (English-to-Filipino) using Bible Dataset\n",
    "\n",
    "def preprocess_function1(bibledataset):\n",
    "    inputs = [x[\"en\"] for x in bibledataset[\"translation\"]]\n",
    "    targets = [y[\"tl\"] for y in bibledataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v3_en2fil(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v3_en2fil.as_target_tokenizer():\n",
    "        labels = tokenizer_v3_en2fil(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_bible_dataset1 = bible_dataset.map(\n",
    "    preprocess_function1,\n",
    "    batched=True,\n",
    "    remove_columns=bible_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_bible_dataset1.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v3_en2fil = DataLoader(\n",
    "    tokenized_bible_dataset1[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v3_en2fil, batch_size=48\n",
    ")\n",
    "\n",
    "model_v3_en2fil, optimizer_v3_en2fil, eval_dataloader_v3_en2fil = accelerator.prepare(\n",
    "    model_v3_en2fil, optimizer_v3_en2fil, eval_dataloader_v3_en2fil\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v3_en2fil.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v3_en2fil.pad_token_id)\n",
    "    decoded_labels = tokenizer_v3_en2fil.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v3_en2fil.eval()\n",
    "for batch in tqdm(eval_dataloader_v3_en2fil):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v3_en2fil).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v3_en2fil.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v3_en2fil = metric.compute()\n",
    "print(f\"BLEU score: {results_v3_en2fil['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v3_en2fil['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v3_en2fil['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69c852398b148d5be9eaabe40a685b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb75757d1ea4caab0091e95e3900e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271b8813f7c54f3fb755bf3249a52dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 59.15\n",
      "Precisions: [80.0895008605852, 64.47565409567264, 53.784463005395814, 45.61430435116261]\n",
      "Brevity Penalty: 0.99\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v3 Transformer (Filipino-to-English) using Bible Dataset\n",
    "\n",
    "def preprocess_function2(bibledataset):\n",
    "    inputs = [x[\"tl\"] for x in bibledataset[\"translation\"]]\n",
    "    targets = [y[\"en\"] for y in bibledataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v3_fil2en(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v3_fil2en.as_target_tokenizer():\n",
    "        labels = tokenizer_v3_fil2en(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_bible_dataset2 = bible_dataset.map(\n",
    "    preprocess_function2,\n",
    "    batched=True,\n",
    "    remove_columns=bible_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_bible_dataset2.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v3_fil2en = DataLoader(\n",
    "    tokenized_bible_dataset2[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v3_fil2en, batch_size=48\n",
    ")\n",
    "\n",
    "model_v3_fil2en, optimizer_v3_fil2en, eval_dataloader_v3_fil2en = accelerator.prepare(\n",
    "    model_v3_fil2en, optimizer_v3_fil2en, eval_dataloader_v3_fil2en\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v3_fil2en.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v3_fil2en.pad_token_id)\n",
    "    decoded_labels = tokenizer_v3_fil2en.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v3_fil2en.eval()\n",
    "for batch in tqdm(eval_dataloader_v3_fil2en):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v3_fil2en).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v3_fil2en.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v3_fil2en = metric.compute()\n",
    "print(f\"BLEU score: {results_v3_fil2en['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v3_fil2en['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v3_fil2en['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c329e9b332b4558818c64f7b345f942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3963bf376e4987a3925a2fdc208bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e909567fce2c4962bf08208bf6099312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 40.15\n",
      "Precisions: [66.9912465608786, 45.137650555294854, 33.52748688452313, 25.622672960057628]\n",
      "Brevity Penalty: 1.00\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v3 Transformer (English-to-Filipino) using Combined Dataset\n",
    "\n",
    "def preprocess_function3(combineddataset):\n",
    "    inputs = [x[\"en\"] for x in combineddataset[\"translation\"]]\n",
    "    targets = [y[\"tl\"] for y in combineddataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v3_en2fil(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v3_en2fil.as_target_tokenizer():\n",
    "        labels = tokenizer_v3_en2fil(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_combined_dataset1 = combined_dataset.map(\n",
    "    preprocess_function3,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_combined_dataset1.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v3_en2fil = DataLoader(\n",
    "    tokenized_combined_dataset1[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v3_en2fil, batch_size=48\n",
    ")\n",
    "\n",
    "model_v3_en2fil, optimizer_v3_en2fil, eval_dataloader_v3_en2fil = accelerator.prepare(\n",
    "    model_v3_en2fil, optimizer_v3_en2fil, eval_dataloader_v3_en2fil\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v3_en2fil.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v3_en2fil.pad_token_id)\n",
    "    decoded_labels = tokenizer_v3_en2fil.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v3_en2fil.eval()\n",
    "for batch in tqdm(eval_dataloader_v3_en2fil):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v3_en2fil).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v3_en2fil.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v3_en2fil = metric.compute()\n",
    "print(f\"BLEU score: {results_v3_en2fil['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v3_en2fil['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v3_en2fil['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d4da80c0314df6b83bba9a6dee73b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7786304264447bb125f40156e32171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18b7a3f1e3e4969a98885cd423116ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 39.92\n",
      "Precisions: [68.67504890118707, 46.87815247153769, 34.97988827338837, 26.730042656916513]\n",
      "Brevity Penalty: 0.96\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v3 Transformer (Filipino-to-English) using Combined Dataset\n",
    "\n",
    "def preprocess_function4(combineddataset):\n",
    "    inputs = [x[\"tl\"] for x in combineddataset[\"translation\"]]\n",
    "    targets = [y[\"en\"] for y in combineddataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v3_fil2en(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v3_fil2en.as_target_tokenizer():\n",
    "        labels = tokenizer_v3_fil2en(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_combined_dataset2 = combined_dataset.map(\n",
    "    preprocess_function4,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_combined_dataset2.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v3_fil2en = DataLoader(\n",
    "    tokenized_combined_dataset2[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v3_fil2en, batch_size=48\n",
    ")\n",
    "\n",
    "model_v3_fil2en, optimizer_v3_fil2en, eval_dataloader_v3_fil2en = accelerator.prepare(\n",
    "    model_v3_fil2en, optimizer_v3_fil2en, eval_dataloader_v3_fil2en\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v3_fil2en.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v3_fil2en.pad_token_id)\n",
    "    decoded_labels = tokenizer_v3_fil2en.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v3_fil2en.eval()\n",
    "for batch in tqdm(eval_dataloader_v3_fil2en):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v3_fil2en).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v3_fil2en.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v3_fil2en = metric.compute()\n",
    "print(f\"BLEU score: {results_v3_fil2en['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v3_fil2en['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v3_fil2en['bp']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e4c2dc8d6f4da25b3aee772c4b12daf1752ba95fe6ef91eb6a3e028a5ce7d28"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05a3867fc7cb355168cd3069bbfa61f57b49db580b0a4f5d5e33f8a392c67a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
