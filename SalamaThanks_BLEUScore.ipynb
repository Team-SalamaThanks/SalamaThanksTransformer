{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AdamW\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\Documents\\SalamaThanks\\.env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Importing SalamaThanks Transformer v2 for English-to-Filipino Translation\n",
    "\n",
    "model_v2_checkpoint_en2fil = \"./v2/en_to_fil/v2.0\"\n",
    "tokenizer_v2_en2fil = AutoTokenizer.from_pretrained(model_v2_checkpoint_en2fil, return_tensors=\"tf\")\n",
    "model_v2_en2fil = AutoModelForSeq2SeqLM.from_pretrained(model_v2_checkpoint_en2fil)\n",
    "data_collator_v2_en2fil = DataCollatorForSeq2Seq(tokenizer_v2_en2fil, model=model_v2_en2fil)\n",
    "optimizer_v2_en2fil = AdamW(model_v2_en2fil.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing SalamaThanks Transformer v2 for Filipino-to-English Translation\n",
    "\n",
    "model_v2_checkpoint_fil2en = \"./v2/fil_to_en/v2.0\"\n",
    "tokenizer_v2_fil2en = AutoTokenizer.from_pretrained(model_v2_checkpoint_fil2en, return_tensors=\"tf\")\n",
    "model_v2_fil2en = AutoModelForSeq2SeqLM.from_pretrained(model_v2_checkpoint_fil2en)\n",
    "data_collator_v2_fil2en = DataCollatorForSeq2Seq(tokenizer_v2_fil2en, model=model_v2_fil2en)\n",
    "optimizer_v2_fil2en = AdamW(model_v2_fil2en.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading sacreBLEU to evaluate BLEU Score and defining length of dataset sentences.\n",
    "\n",
    "metric = load_metric('sacrebleu')\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset bible_para (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\bible_para\\en-tl-lang1=en,lang2=tl\\0.0.0\\b6cc20bcbfb0299beeba1dcc80a8420b975938ca0eef75b3ed30b50df7d950b1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a31bae503e455fb8e8fdff0979eca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading bible_para dataset from Huggingface\n",
    "\n",
    "raw_bible_dataset = load_dataset('bible_para', lang1='en', lang2='tl')\n",
    "raw_bible_dataset = raw_bible_dataset.remove_columns(['id'])\n",
    "bible_dataset = raw_bible_dataset['train'].train_test_split(train_size=0.92, test_size=0.08)\n",
    "bible_dataset[\"validation\"] = bible_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset tatoeba (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\tatoeba\\en-tl-lang1=en,lang2=tl\\0.0.0\\b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36c285b80d447ed8715662baa1c66a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2014-e2e7a3ad21b13d2b\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2014-e2e7a3ad21b13d2b\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c315fcf24249f4a920643fee22e1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2015-e6db70baf321c7f2\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2015-e6db70baf321c7f2\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfa6cf3cf7748eb9a588b62c5ee37dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en_tl_2016-9907aa0a67465528\n",
      "Reusing dataset ted_talks_iwslt (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\ted_talks_iwslt\\en_tl_2016-9907aa0a67465528\\1.1.0\\43935b3fe470c753a023642e1f54b068c590847f9928bd3f2ec99f15702ad6a6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24ab5d3e435405b8e5849444d438c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_gnome (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\opus_gnome\\en-tl-lang1=en,lang2=tl\\0.0.0\\c00e5dfb1b3b508d7898e160feee1d391e67a3651a06570b45d54ab6a8886217)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3960a03d4214d0ebeb99bb50fcf5429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_para_crawl (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\opus_para_crawl\\en-tl-lang1=en,lang2=tl\\0.0.0\\d0becb3ac754eb295ccf6b4b87f391d12d2f4217dbc4f87f2a9718ba1f2de4a3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de256ab876943c1a5010cc71cc33a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset open_subtitles (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\open_subtitles\\en-tl-lang1=en,lang2=tl\\0.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cde7dc282f4074a396824f9b51764a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset opus_ubuntu (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\opus_ubuntu\\en-tl-lang1=en,lang2=tl\\0.0.0\\7ac83b46edf6d0b6ff96bc86d5aadfb8b877c2f136a94af490988c442d3814b8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e1a60d1ce140d3b70047ceeede6242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset multi_para_crawl (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\multi_para_crawl\\en-tl-lang1=en,lang2=tl\\0.0.0\\923bd780ac54acc2d7228bf36806e2a2309aaab30aa3bee613145aaff39eb83c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1482a16ff8c34c89b35fd6f44bbe00d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-tl-lang1=en,lang2=tl\n",
      "Reusing dataset qed_amara (C:\\Users\\Peter\\.cache\\huggingface\\datasets\\qed_amara\\en-tl-lang1=en,lang2=tl\\0.0.0\\3662cb8fbbe21ebafc420ac0a1b3d1898312661d4f898adc79149fa09d073ba0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e25ef90d254d1e8e5a33572c7d0008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading datasets from Huggingface and combining them into one dataset\n",
    "\n",
    "raw_tatoeba_dataset = load_dataset('tatoeba', lang1='en', lang2='tl')\n",
    "raw_tatoeba_dataset = raw_tatoeba_dataset.remove_columns(['id'])\n",
    "raw_tedtalks1_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2014\")\n",
    "raw_tedtalks2_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2015\")\n",
    "raw_tedtalks3_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2016\")\n",
    "raw_gnome_dataset = load_dataset('opus_gnome', lang1='en', lang2='tl')\n",
    "raw_gnome_dataset = raw_gnome_dataset.remove_columns(['id'])\n",
    "raw_paracrawl_dataset = load_dataset('opus_paracrawl', lang1='en', lang2='tl')\n",
    "raw_paracrawl_dataset = raw_paracrawl_dataset.remove_columns(['id'])\n",
    "raw_subtitles_dataset = load_dataset(\"open_subtitles\", lang1=\"en\", lang2=\"tl\")\n",
    "raw_subtitles_dataset = raw_subtitles_dataset.remove_columns(['id'])\n",
    "raw_subtitles_dataset = raw_subtitles_dataset.remove_columns(['meta'])\n",
    "raw_ubuntu_dataset = load_dataset('opus_ubuntu', lang1='en', lang2='tl')\n",
    "raw_ubuntu_dataset = raw_ubuntu_dataset.remove_columns(['id'])\n",
    "raw_multiparacrawl_dataset = load_dataset('multi_para_crawl', lang1='en', lang2='tl')\n",
    "raw_multiparacrawl_dataset = raw_multiparacrawl_dataset.remove_columns(['id'])\n",
    "raw_qedamara_dataset = load_dataset('qed_amara', lang1 = 'en', lang2 = 'tl')\n",
    "raw_qedamara_dataset = raw_qedamara_dataset.remove_columns(['id'])\n",
    "\n",
    "\n",
    "raw_combined_dataset = concatenate_datasets([raw_tatoeba_dataset['train'], raw_tedtalks1_dataset['train'], raw_tedtalks2_dataset['train'], raw_tedtalks3_dataset['train'], raw_gnome_dataset['train'], raw_paracrawl_dataset['train'], raw_subtitles_dataset['train'], raw_ubuntu_dataset['train'], raw_multiparacrawl_dataset['train'], raw_qedamara_dataset['train']])\n",
    "\n",
    "combined_dataset = raw_combined_dataset.train_test_split(train_size=0.92, test_size=0.08)\n",
    "combined_dataset[\"validation\"] = combined_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 57219\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 4976\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 489822\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 42594\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0e3f29047c4121b8aacef808f1dd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff3b2debfe649be9e2d640c0c0d0adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928ca36f292c40c8a42590f5d2fc2b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 69.90\n",
      "Precisions: [86.12221965165085, 73.83548067393458, 64.96727401379799, 57.78723092155928]\n",
      "Brevity Penalty: 1.00\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v2 Transformer (English-to-Filipino) using Bible Dataset\n",
    "\n",
    "def preprocess_function1(bibledataset):\n",
    "    inputs = [x[\"en\"] for x in bibledataset[\"translation\"]]\n",
    "    targets = [y[\"tl\"] for y in bibledataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v2_en2fil(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v2_en2fil.as_target_tokenizer():\n",
    "        labels = tokenizer_v2_en2fil(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_bible_dataset1 = bible_dataset.map(\n",
    "    preprocess_function1,\n",
    "    batched=True,\n",
    "    remove_columns=bible_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_bible_dataset1.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v2_en2fil = DataLoader(\n",
    "    tokenized_bible_dataset1[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v2_en2fil, batch_size=48\n",
    ")\n",
    "\n",
    "model_v2_en2fil, optimizer_v2_en2fil, eval_dataloader_v2_en2fil = accelerator.prepare(\n",
    "    model_v2_en2fil, optimizer_v2_en2fil, eval_dataloader_v2_en2fil\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v2_en2fil.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v2_en2fil.pad_token_id)\n",
    "    decoded_labels = tokenizer_v2_en2fil.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v2_en2fil.eval()\n",
    "for batch in tqdm(eval_dataloader_v2_en2fil):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v2_en2fil).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v2_en2fil.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v2_en2fil = metric.compute()\n",
    "print(f\"BLEU score: {results_v2_en2fil['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v2_en2fil['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v2_en2fil['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a17bf726b440069cd8b1595df267ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c93e756e1543ca8c4ab4a742d7d505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2167d566dc8475e8c2766bd43abe681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 59.93\n",
      "Precisions: [80.57468034284109, 65.49213744903902, 55.0120864178879, 47.119309262166404]\n",
      "Brevity Penalty: 0.99\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v2 Transformer (Filipino-to-English) using Bible Dataset\n",
    "\n",
    "def preprocess_function2(bibledataset):\n",
    "    inputs = [x[\"tl\"] for x in bibledataset[\"translation\"]]\n",
    "    targets = [y[\"en\"] for y in bibledataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v2_fil2en(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v2_fil2en.as_target_tokenizer():\n",
    "        labels = tokenizer_v2_fil2en(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_bible_dataset2 = bible_dataset.map(\n",
    "    preprocess_function2,\n",
    "    batched=True,\n",
    "    remove_columns=bible_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_bible_dataset2.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v2_fil2en = DataLoader(\n",
    "    tokenized_bible_dataset2[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v2_fil2en, batch_size=48\n",
    ")\n",
    "\n",
    "model_v2_fil2en, optimizer_v2_fil2en, eval_dataloader_v2_fil2en = accelerator.prepare(\n",
    "    model_v2_fil2en, optimizer_v2_fil2en, eval_dataloader_v2_fil2en\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v2_fil2en.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v2_fil2en.pad_token_id)\n",
    "    decoded_labels = tokenizer_v2_fil2en.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v2_fil2en.eval()\n",
    "for batch in tqdm(eval_dataloader_v2_fil2en):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v2_fil2en).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v2_fil2en.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v2_fil2en = metric.compute()\n",
    "print(f\"BLEU score: {results_v2_fil2en['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v2_fil2en['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v2_fil2en['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ae37eb657347a4b8861aa4593b4538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86cfee2d42e4f2ab5d5ed5a59505aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571512b2dad547d9b1fc87276dc63036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 29.03\n",
      "Precisions: [58.78027648497581, 33.65989370935334, 22.46870744951359, 15.986098306049701]\n",
      "Brevity Penalty: 1.00\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v2 Transformer (English-to-Filipino) using Combined Dataset\n",
    "\n",
    "def preprocess_function3(combineddataset):\n",
    "    inputs = [x[\"en\"] for x in combineddataset[\"translation\"]]\n",
    "    targets = [y[\"tl\"] for y in combineddataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v2_en2fil(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v2_en2fil.as_target_tokenizer():\n",
    "        labels = tokenizer_v2_en2fil(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_combined_dataset1 = combined_dataset.map(\n",
    "    preprocess_function3,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_combined_dataset1.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v2_en2fil = DataLoader(\n",
    "    tokenized_combined_dataset1[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v2_en2fil, batch_size=48\n",
    ")\n",
    "\n",
    "model_v2_en2fil, optimizer_v2_en2fil, eval_dataloader_v2_en2fil = accelerator.prepare(\n",
    "    model_v2_en2fil, optimizer_v2_en2fil, eval_dataloader_v2_en2fil\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v2_en2fil.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v2_en2fil.pad_token_id)\n",
    "    decoded_labels = tokenizer_v2_en2fil.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v2_en2fil.eval()\n",
    "for batch in tqdm(eval_dataloader_v2_en2fil):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v2_en2fil).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v2_en2fil.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v2_en2fil = metric.compute()\n",
    "print(f\"BLEU score: {results_v2_en2fil['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v2_en2fil['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v2_en2fil['bp']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a6af094965450bb1cb985b001deb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6a1b6df40f4504a9ba1728fa2f78c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d766376db94a88b46fd514d041c2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 31.87\n",
      "Precisions: [62.414873741939374, 38.27368477498291, 26.430754368201537, 19.014448757426198]\n",
      "Brevity Penalty: 0.96\n"
     ]
    }
   ],
   "source": [
    "#BLEU Evaluation of SalamaThanks v2 Transformer (Filipino-to-English) using Combined Dataset\n",
    "\n",
    "def preprocess_function4(combineddataset):\n",
    "    inputs = [x[\"tl\"] for x in combineddataset[\"translation\"]]\n",
    "    targets = [y[\"en\"] for y in combineddataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer_v2_fil2en(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer_v2_fil2en.as_target_tokenizer():\n",
    "        labels = tokenizer_v2_fil2en(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_combined_dataset2 = combined_dataset.map(\n",
    "    preprocess_function4,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_combined_dataset2.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader_v2_fil2en = DataLoader(\n",
    "    tokenized_combined_dataset2[\"validation\"].shard(num_shards=5, index=0), collate_fn=data_collator_v2_fil2en, batch_size=48\n",
    ")\n",
    "\n",
    "model_v2_fil2en, optimizer_v2_fil2en, eval_dataloader_v2_fil2en = accelerator.prepare(\n",
    "    model_v2_fil2en, optimizer_v2_fil2en, eval_dataloader_v2_fil2en\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer_v2_fil2en.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_v2_fil2en.pad_token_id)\n",
    "    decoded_labels = tokenizer_v2_fil2en.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "model_v2_fil2en.eval()\n",
    "for batch in tqdm(eval_dataloader_v2_fil2en):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model_v2_fil2en).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "        )\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer_v2_fil2en.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "results_v2_fil2en = metric.compute()\n",
    "print(f\"BLEU score: {results_v2_fil2en['score']:.2f}\")\n",
    "print(\"Precisions:\", list(results_v2_fil2en['precisions']))\n",
    "print(f\"Brevity Penalty: {results_v2_fil2en['bp']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05a3867fc7cb355168cd3069bbfa61f57b49db580b0a4f5d5e33f8a392c67a8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05a3867fc7cb355168cd3069bbfa61f57b49db580b0a4f5d5e33f8a392c67a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
