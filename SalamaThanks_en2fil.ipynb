{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary APIs and transformer to use, and loading necessary variables\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AdamW, get_scheduler\n",
    "\n",
    "model_checkpoint = \"./v2/en_to_fil/v2.0\"\n",
    "translator_en2fil = pipeline(\"translation\", model=model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"tf\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset from HuggingFace and sacreBLEU to evaluate BLEU Score\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric\n",
    "\n",
    "raw_tatoeba_dataset = load_dataset('tatoeba', lang1='en', lang2='tl')\n",
    "raw_tatoeba_dataset = raw_tatoeba_dataset.remove_columns(['id'])\n",
    "raw_tedtalks1_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2014\")\n",
    "raw_tedtalks2_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2015\")\n",
    "raw_tedtalks3_dataset = load_dataset('ted_talks_iwslt', language_pair=(\"en\", \"tl\"), year=\"2016\")\n",
    "raw_gnome_dataset = load_dataset('opus_gnome', lang1='en', lang2='tl')\n",
    "raw_gnome_dataset = raw_gnome_dataset.remove_columns(['id'])\n",
    "raw_paracrawl_dataset = load_dataset('opus_paracrawl', lang1='en', lang2='tl')\n",
    "raw_paracrawl_dataset = raw_paracrawl_dataset.remove_columns(['id'])\n",
    "raw_subtitles_dataset = load_dataset(\"open_subtitles\", lang1=\"en\", lang2=\"tl\")\n",
    "raw_subtitles_dataset = raw_subtitles_dataset.remove_columns(['id'])\n",
    "raw_subtitles_dataset = raw_subtitles_dataset.remove_columns(['meta'])\n",
    "raw_ubuntu_dataset = load_dataset('opus_ubuntu', lang1='en', lang2='tl')\n",
    "raw_ubuntu_dataset = raw_ubuntu_dataset.remove_columns(['id'])\n",
    "raw_multiparacrawl_dataset = load_dataset('multi_para_crawl', lang1='en', lang2='tl')\n",
    "raw_multiparacrawl_dataset = raw_multiparacrawl_dataset.remove_columns(['id'])\n",
    "raw_qedamara_dataset = load_dataset('qed_amara', lang1 = 'en', lang2 = 'tl')\n",
    "raw_qedamara_dataset = raw_qedamara_dataset.remove_columns(['id'])\n",
    "\n",
    "\n",
    "raw_combined_dataset = concatenate_datasets([raw_tatoeba_dataset['train'], raw_tedtalks1_dataset['train'], raw_tedtalks2_dataset['train'], raw_tedtalks3_dataset['train'], raw_gnome_dataset['train'], raw_paracrawl_dataset['train'], raw_subtitles_dataset['train'], raw_ubuntu_dataset['train'], raw_multiparacrawl_dataset['train'], raw_qedamara_dataset['train']])\n",
    "\n",
    "combined_dataset = raw_combined_dataset.train_test_split(train_size=0.92, test_size=0.08)\n",
    "combined_dataset[\"validation\"] = combined_dataset.pop('test')\n",
    "\n",
    "metric = load_metric('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to tokenize dataset\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "\n",
    "def preprocess_function(combineddataset):\n",
    "    inputs = [x[\"en\"] for x in combineddataset[\"translation\"]]\n",
    "    targets = [y[\"tl\"] for y in combineddataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of dataset\n",
    "\n",
    "tokenized_combined_dataset = combined_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the DataLoader and setting dataset to return PyTorch tensors\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_combined_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_combined_dataset[\"train\"].shard(num_shards=5, index=0),\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=48,\n",
    ")\n",
    "\n",
    "#eval_dataloader = DataLoader(\n",
    "#    tokenized_combined_dataset[\"validation\"], collate_fn=data_collator, batch_size=48\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Accelerator and preparing settings\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_dataloader\n",
    "    #,eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing scheduler and output directory\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "output_dir = \"./v3/en_to_fil/v3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing post-processing function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and evaluating the transformer using PyTorch and Accelerate and saving to output directory\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    #model.eval()\n",
    "    #for batch in tqdm(eval_dataloader):\n",
    "    #    with torch.no_grad():\n",
    "    #        generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "    #            batch[\"input_ids\"],\n",
    "    #            attention_mask=batch[\"attention_mask\"],\n",
    "    #            max_length=256,\n",
    "    #        )\n",
    "    #    labels = batch[\"labels\"]\n",
    "\n",
    "    #    generated_tokens = accelerator.pad_across_processes(\n",
    "    #        generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "    #    )\n",
    "    #    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    #    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    #    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    #    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    #    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    #results = metric.compute()\n",
    "    #print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05a3867fc7cb355168cd3069bbfa61f57b49db580b0a4f5d5e33f8a392c67a8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05a3867fc7cb355168cd3069bbfa61f57b49db580b0a4f5d5e33f8a392c67a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
